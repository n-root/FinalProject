---
title: "Modeling"
format: html
editor: visual
---

# Final Project Modeling File - Natalie Root

## Establish Libraries

```{r}
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(httr)
library(jsonlite)
library(knitr)
library(rpart)
library(randomForest, lib.loc="C:/Users/natal/Desktop/R_packages")
library(gbm, lib.loc="C:/Users/natal/Desktop/R_packages")
library(caret, lib.loc="C:/Users/natal/Desktop/R_packages")
```

## Introduction


## LogLoss Research

#### What is logLoss?


#### Why may we prefer it to things like accuracy when we have a binary response variable?


## Split the Data

```{r}
# Set a seed in order to make things reproducible
set.seed(14)
```

```{r}
# Ensure NoDocbcCost is a factor with valid level names
levels(diabetes_data$NoDocbcCost) <- make.names(levels(diabetes_data$NoDocbcCost))
```

```{r}
# Split the data into training and testing sets (70% train, 30% test)
train_index <- createDataPartition(diabetes_data$NoDocbcCost, p = 0.7, list = FALSE)

train_data <- diabetes_data[train_index, ]
test_data <- diabetes_data[-train_index, ]
```

## Logistic Regression Models

```{r}
# Candidate logistic regression model 1
model1 <- glm(NoDocbcCost ~ ., data = train_data, family = binomial)
```

```{r}
# Candidate logistic regression model 2
model2 <- glm(NoDocbcCost ~ . -Age, data = train_data, family = binomial)
```

```{r}
# Candidate logistic regression model 3
model3 <- glm(NoDocbcCost ~ Age + Income, data = train_data, family = binomial)
```

```{r}
# Cross-validation to choose the best logistic regression model with logLoss as metric
control <- trainControl(method = "cv", number = 5, summaryFunction = mnLogLoss, classProbs = TRUE)
```

```{r}
logreg1 <- train(NoDocbcCost ~ ., data = train_data, method = "glm", family = binomial, trControl = control)
```

```{r}
logreg2 <- train(NoDocbcCost ~ . -Age, data = train_data, method = "glm", family = binomial, trControl = control)
```

```{r}
logreg3 <- train(NoDocbcCost ~ Age + Income, data = train_data, method = "glm", family = binomial, trControl = control)
```
```{r}
# Choose the best logistic regression model
best_logreg <- logreg1

if (logreg2$results$logLoss < best_logreg$results$logLoss) best_logreg <- logreg2
if (logreg3$results$logLoss < best_logreg$results$logLoss) best_logreg <- logreg3
```


## Classification Tree


```{r}
# Fit a classification tree with varying values for the complexity parameter
treeGrid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
tree_model <- train(NoDocbcCost ~ ., data = train_data, method = "rpart", trControl = control, tuneGrid = treeGrid)
```

```{r}
# Best tree model based on complexity parameter
best_treemodel <- tree_model$finalModel
```

## Random Forest

```{r}
# Fit a random forest model
rfGrid <- expand.grid(mtry = c(1, 2, 3))
rfModel <- train(NoDocbcCost ~ ., data = train_data, method = "rf", trControl = control, tuneGrid = rfGrid, ntree = 100)
```

```{r}
# Best tree model based on complexity parameter
best_rf <- rfModel$finalModel
```

## Final Model Selection

```{r}
# Compare all 3 models on the test set
pred_logreg <- predict(best_logreg, newdata = test_data, type = "prob")[,2]
pred_tree <- predict(best_treemodel, newdata = test_data, type = "prob")[,2]
pred_rf <- predict(best_rf, newdata = test_data, type = "prob")[,2]
```

```{r}
# Calculate log loss for each model
logloss_logreg <- mnLogLoss(data.frame(obs = test_data$NoDocbcCost, pred = ifelse(pred_logreg > 0.5, 1, 0), Yes = pred_logreg, No = 1 - pred_logreg))

logloss_tree <- mnLogLoss(data.frame(obs = test_data$NoDocbcCost, pred = ifelse(pred_tree > 0.5, 1, 0), Yes = pred_tree, No = 1 - pred_tree))

logloss_rf <- mnLogLoss(data.frame(obs = test_data$NoDocbcCost, pred = ifelse(pred_rf > 0.5, 1, 0), Yes = pred_rf, No = 1 - pred_rf))
```

```{r}
# Identify overall winner
winner <- "Logistic Regression"
if (logloss_tree < logloss_logreg && logloss_tree < logloss_rf) winner <- "Classification Tree"
if (logloss_rf < logloss_logreg && logloss_rf < logloss_tree) winner <- "Random Forest"

cat("The best model is", winner)
```

