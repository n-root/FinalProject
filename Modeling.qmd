---
title: "Modeling"
format: html
editor: visual
---

# Final Project Modeling File - Natalie Root

## Establish Libraries

```{r}
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(httr)
library(jsonlite)
library(knitr)
library(rpart)
library(randomForest, lib.loc="C:/Users/natal/Desktop/R_packages")
library(gbm, lib.loc="C:/Users/natal/Desktop/R_packages")
library(caret, lib.loc="C:/Users/natal/Desktop/R_packages")
```

## Introduction


## LogLoss Research

#### What is logLoss?


#### Why may we prefer it to things like accuracy when we have a binary response variable?


## Split the Data

```{r}
# Set a seed in order to make things reproducible
set.seed(14)
```

```{r}
# Ensure NoDocbcCost is a factor with valid level names
levels(diabetes_data$NoDocbcCost) <- make.names(levels(diabetes_data$NoDocbcCost))
```

```{r}
# Split the data into training and testing sets (70% train, 30% test)
train_index <- createDataPartition(diabetes_data$NoDocbcCost, p = 0.7, list = FALSE)

train_data <- diabetes_data[train_index, ]
test_data <- diabetes_data[-train_index, ]
```

## Predict the Diabetes_binary variable

```{r}
#use caret()

# Use logLoss as metric to evaluate models

# For all model types, use logLoss with 5 fold cross-validation to select the best model

# Set up own grid of tuning parameters in any model where that is possible
```

## Logistic Regression Models

```{r}
# Candidate logistic regression model 1
model1 <- glm(NoDocbcCost ~ ., data = train_data, family = binomial)
```

```{r}
# Candidate logistic regression model 2
model2 <- glm(NoDocbcCost ~ . -Age, data = train_data, family = binomial)
```

```{r}
# Candidate logistic regression model 3
model3 <- glm(NoDocbcCost ~ Age + Income, data = train_data, family = binomial)
```

```{r}
# Cross-validation to choose the best logistic regression model with logLoss as metric
control <- trainControl(method = "cv", number = 10, summaryFunction = mnLogLoss, classProbs = TRUE)
```
```{r}
logreg1 <- train(NoDocbcCost ~ ., data = train_data, method = "glm", family = binomial, trControl = control)
```

```{r}
logreg2 <- train(NoDocbcCost ~ . -Age, data = train_data, method = "glm", family = binomial, trControl = control)
```

```{r}
logreg3 <- train(NoDocbcCost ~ Age + Income, data = train_data, method = "glm", family = binomial, trControl = control)
```
```{r}
# Choose the best logistic regression model
best_logreg <- logreg1

if (logreg2$results$logLoss < best_logreg$results$logLoss) best_logreg <- logreg2
if (logreg3$results$logLoss < best_logreg$results$logLoss) best_logreg <- logreg3
```


## Classification Tree


```{r}
# Fit a classification tree with varying values for the complexity parameter
treeGrid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
tree_model <- train(NoDocbcCost ~ ., data = train_data, method = "rpart", trControl = control, tuneGrid = treeGrid)
```

```{r}
# Best tree model based on complexity parameter
best_treemodel <- tree_model$finalModel
```

## Random Forest


## Final Model Selection



