---
title: "Modeling"
format: html
editor: visual
---

# Final Project Modeling File - Natalie Root

## Establish Libraries

```{r}
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(httr)
library(jsonlite)
library(knitr)
library(rpart)
library(randomForest, lib.loc="C:/Users/natal/Desktop/R_packages")
library(gbm, lib.loc="C:/Users/natal/Desktop/R_packages")
library(caret, lib.loc="C:/Users/natal/Desktop/R_packages")
```

## Introduction

The data set that we are utilizing throughout this project is formed from the Behavioral Risk Factor Surveillance System (BRFSS) annual telephone survey from the CDC. The survey is composed of 253,680 responses regarding health related risk behaviors, chronic health conditions, and preventative service use from 2015. In this file, we will evaluate three different types of models (logistic regression, classification tree, and random forest) and their predictive performance on the variables 'NoDocbcCost', 'Age', and 'Income.' We will identify the best models within each of these three categories and then finally identify the best model between all three categories.

## LogLoss Research

#### What is logLoss?
Log loss, otherwise known as logarithmic loss, is a performance metric that evaluates the predictions of classification models. It does this by measuring the accurancy of the model's prediction probabilities. The lower the log loss value, the higher the accuracy, and vice versa.

#### Why may we prefer it to things like accuracy when we have a binary response variable?
We may prefer log loss to things like accuracy when we have a binary response variable because it evaluates the quality of the prediction probabilities, penalizes incorrect predictions severely, and provides continuous feedback on the predictions. Overall, log loss provides a more comprehensive evaluation of statistical models and their ability to predict.

## Split the Data

```{r}
# Set a seed in order to make things reproducible
set.seed(14)
```

```{r}
# Ensure NoDocbcCost is a factor with valid level names
levels(diabetes_data$NoDocbcCost) <- make.names(levels(diabetes_data$NoDocbcCost))
```

```{r}
# Split the data into training and testing sets (70% train, 30% test)
train_index <- createDataPartition(diabetes_data$NoDocbcCost, p = 0.7, list = FALSE)

train_data <- diabetes_data[train_index, ]
test_data <- diabetes_data[-train_index, ]
```

## Logistic Regression Models

A logistic regression model is a statistical model that is best suited for binary classification. These models can predict the probability that an object belongs to a class. The data set we are using contains a large number of binary variables, therefore making logistic regression models a reasonable model for us to apply.

```{r}
# Candidate logistic regression model 1
model1 <- glm(NoDocbcCost ~ ., data = train_data, family = binomial)
```

```{r}
# Candidate logistic regression model 2
model2 <- glm(NoDocbcCost ~ . -Age, data = train_data, family = binomial)
```

```{r}
# Candidate logistic regression model 3
model3 <- glm(NoDocbcCost ~ Age + Income, data = train_data, family = binomial)
```

```{r}
# Cross-validation to choose the best logistic regression model with logLoss as metric
control <- trainControl(method = "cv", number = 5, summaryFunction = mnLogLoss, classProbs = TRUE)
```

```{r}
logreg1 <- train(NoDocbcCost ~ ., data = train_data, method = "glm", family = binomial, trControl = control)
```

```{r}
logreg2 <- train(NoDocbcCost ~ . -Age, data = train_data, method = "glm", family = binomial, trControl = control)
```

```{r}
logreg3 <- train(NoDocbcCost ~ Age + Income, data = train_data, method = "glm", family = binomial, trControl = control)
```
```{r}
# Choose the best logistic regression model
best_logreg <- logreg1

if (logreg2$results$logLoss < best_logreg$results$logLoss) best_logreg <- logreg2
if (logreg3$results$logLoss < best_logreg$results$logLoss) best_logreg <- logreg3
```


## Classification Tree

A classification tree model is a statistical model that is used for classifying both categorical and numerical (continuous) data into classes. We may try to use classification trees because they are simple and interpretable models.

```{r}
# Fit a classification tree with varying values for the complexity parameter
treeGrid <- expand.grid(cp = seq(0.01, 0.1, by = 0.01))
tree_model <- train(NoDocbcCost ~ ., data = train_data, method = "rpart", trControl = control, tuneGrid = treeGrid)
```

```{r}
# Best tree model based on complexity parameter
best_treemodel <- tree_model$finalModel
```

## Random Forest

A random forest model is a statistical model that combines multiple decision trees in order to improve the performance of the model. This also produces a stronger and more robust model than some of our other options due to the number of decision trees. We may use these models for complex classification tasks.

```{r}
# Fit a random forest model
rfGrid <- expand.grid(mtry = c(1, 2, 3))
rfModel <- train(NoDocbcCost ~ ., data = train_data, method = "rf", trControl = control, tuneGrid = rfGrid, ntree = 100)
```

```{r}
# Best tree model based on complexity parameter
best_rf <- rfModel$finalModel
```

## Final Model Selection

```{r}
# Compare all 3 models on the test set
pred_logreg <- predict(best_logreg, newdata = test_data, type = "prob")[,2]
pred_tree <- predict(best_treemodel, newdata = test_data, type = "prob")[,2]
pred_rf <- predict(best_rf, newdata = test_data, type = "prob")[,2]
```

```{r}
# Calculate log loss for each model
logloss_logreg <- mnLogLoss(data.frame(obs = test_data$NoDocbcCost, pred = ifelse(pred_logreg > 0.5, 1, 0), Yes = pred_logreg, No = 1 - pred_logreg))

logloss_tree <- mnLogLoss(data.frame(obs = test_data$NoDocbcCost, pred = ifelse(pred_tree > 0.5, 1, 0), Yes = pred_tree, No = 1 - pred_tree))

logloss_rf <- mnLogLoss(data.frame(obs = test_data$NoDocbcCost, pred = ifelse(pred_rf > 0.5, 1, 0), Yes = pred_rf, No = 1 - pred_rf))
```

```{r}
# Identify overall winner
winner <- "Logistic Regression"
if (logloss_tree < logloss_logreg && logloss_tree < logloss_rf) winner <- "Classification Tree"
if (logloss_rf < logloss_logreg && logloss_rf < logloss_tree) winner <- "Random Forest"

cat("The best model is", winner)
```

